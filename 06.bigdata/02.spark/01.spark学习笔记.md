[TOC]

#spark学习笔记

##未解决问题汇总



##tips

- 如果在eclipse上直接调试呢？ conf.setMaster("local");可以直接在本地调试  

##spark安装

###下载spark1.6.1
[下载地址](http://apache.opencas.org/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz)
###上传spark
###安装java
参见[jdk安装](../../03.linux/01.centos/02.java相关配置/01.jdk安装和环境变量配置.md)
###安装scala
[scala安装](../../03.linux/01.centos/06.常用工具配置/02.scala安装配置.md)
###安装hadoop
[hadoop分布式安装](../01.hadoop/04.hadoop分布式安装.md)  
[hadoop伪分布式安装](../01.hadoop/02.hadoop伪分布式安装.md)  

###配置环境变量
	
	vim ~/.bashrc
	SPARK_HOME=/home/hadoop/app/spark-1.6.1
	PATH=$PATH:/$SPARK_HOME/bin
	export PATH SPARK_HOME
source ~/.bashrc使配置生效  

###配置spark

[spark伪分布式](01.dir/spark配置文件/1.6.1/伪分布式安装)
[spark分布式](01.dir/spark配置文件/1.6.1/分布式安装)

###修改spark-env.sh

	cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh

伪分布什么都不需要修改

###修改slaves
	
	vim $SPARK_HOME/conf/slaves

	hadoopallinone(根据需要配置)

###启动
启动hdfs  
启动spark  

	$SPARK_HOME/sbin/start-all.sh

###验证
启动浏览器
	
	http://hadoopallinone:8080  #浏览器地址
	$SPARK_HOME/bin/spark-shell  --master spark://hadoopallinone:7077 --name testsparksql --executor-cores 1 --executor-memory 512m   #启动spark-shell 不指定master 默认以local启动

##调试WordCount
### 先看一下WordCount的代码

	import org.apache.spark.SparkConf;
	import org.apache.spark.SparkContext;
	object WordCount {
	  def main(args: Array[String]) { 
	    val conf = new SparkConf();
	    conf.setAppName("WordCount").setMaster("spark://hadoopallinone:7077");
	    val sc = new SparkContext(conf) ;
	    val file = "hdfs://hadoopallinone:9000/user.txt" ;
	    val lines = sc.textFile(file); 
	    val words = lines.flatMap(_.split(" "));
	    val wordCount = words.countByValue();
	    println(wordCount);
	  }
	}

###spark-submit
	
	spark-submit --class WordCount --master spark://hadoopallinone:7077 ~/jars/WordCount.jar 

##spark运行架构

###几个基本概念：

（1）job：包含多个task组成的并行计算，往往由action催生。  

（2）stage：job的调度单位。  

（3）task：被送到某个executor上的工作单元。  

（4）taskSet：一组关联的，相互之间没有shuffle依赖关系的任务组成的任务集。  

一个应用程序由一个driver program和多个job构成。一个job由多个stage组成。一个stage由多个没有shuffle关系的task组成。  

 

###spark应用程序的运行架构

（1）简单的说：  

由driver向集群申请资源，集群分配资源，启动executor。driver将spark应用程序的代码和文件传送给executor。executor上运行task，运行完之后将结果返回给driver或者写入外界。  

（2）复杂点说：  

提交应用程序，构建sparkContext，构建DAG图，提交给scheduler进行解析，解析成一个个stage，提交给集群，由集群任务管理器进行调度，集群启动spark   executor。driver把代码和文件传给executor。executor进行各种运算完成task任务。driver上的block tracker记录executor在各个节点上产生的数据块。task运行完之后，将数据写入HDFS上或者其他类型数据库里。  

（3）全面点说：  

spark应用程序进行各种transformation的计算，最后通过action触发job。提交之后首先通过sparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler进行解析，解析时是以shuffle为边界，反向解析，构建stage，stage之间也有依赖关系。这个过程就是对DAG图进行解析划分stage，并且计算出各个stage之间的依赖关系。然后将一个个TaskSet提交给底层调度器，在spark中是提交给taskScheduler处理，生成TaskSet manager，最后提交给executor进行计算，executor多线程计算，计算完反馈给TaskSetmanager，再反馈给taskScheduler，然后再反馈回DAGScheduler。全部运行完之后写入数据。

（4）更加深入理解：  

应用程序提交后，触发action，构建sparkContext，构建DAG图，提交给DAGScheduler，构建stage，以stageSet方式提交给TaskScheduler，构建taskSet Manager，然后将task提交给executor运行。executor运行完task后，将完成信息提交给schedulerBackend，由它将任务完成的信息提交给TaskScheduler。TaskScheduler反馈信息给TaskSetManager，删除该task任务，执行下一个任务。同时TaskScheduler将完成的结果插入到成功队列里，加入之后返回加入成功的信息。TaskScheduler将任务处理成功的信息传给TaskSet Manager。全部任务完成后TaskSetManager将结果反馈给DAGScheduler。如果属于resultTask，交给JobListener。如果不属于resultTask，保存结果。 

##sparksql汇总

###sparksql和hive结合
spark增加hive的配置
	
	cd $SPARK_HOME/conf/
	vim hive-site.xml
	<configuration>
	<!---->
	<property>
	  <name>hive.metastore.uris</name>
	  <value>thrift://hadoopallinone:9083</value>
	  <description>Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.</description>
	</property>
	</configuration>

启动hive  

	hive --service metastore &

启动spark

	$SPARK_HOME/sbin/start-all.sh

spark sql 

	$SPARK_HOME/bin/spark-sql --master spark://hadoopallinone:7077 --name testsparksql --executor-cores 1 --executor-memory 512m
	show databases;


###sparksql代码开发
和hive结合
	
	import org.apache.spark.sql.SQLContext
	import org.apache.spark.SparkConf
	import org.apache.spark.SparkContext

	val sparkConf = new SparkConf()
    val sc = new SparkContext(sparkConf)
    val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
    val result = sqlContext.sql("show databases").collect.foreach(println) 

内存创建RDD

	import org.apache.spark.{SparkConf, SparkContext}
	import org.apache.spark.sql.SQLContext
	import org.apache.spark.sql.functions._

	// One method for defining the schema of an RDD is to make a case class with the desired column
	// names and types.
	case class Record(key: Int, value: String)

	object RDDRelation {
	  def main(args: Array[String]) {
	    val sparkConf = new SparkConf().setAppName("RDDRelation").setMaster("local");
	    val sc = new SparkContext(sparkConf)
	    val sqlContext = new SQLContext(sc)

	    // Importing the SQL context gives access to all the SQL functions and implicit conversions.
	    import sqlContext.implicits._

	    val df = sc.parallelize((1 to 100).map(i => Record(i, s"val_$i"))).toDF()
	    // Any RDD containing case classes can be registered as a table.  The schema of the table is
	    // automatically inferred using scala reflection.
	    df.registerTempTable("records")

	    // Once tables have been registered, you can run SQL queries over them.
	    println("Result of SELECT *:")
	    sqlContext.sql("SELECT * FROM records").collect().foreach(println)

	    // Aggregation queries are also supported.
	    val count = sqlContext.sql("SELECT COUNT(*) FROM records").collect().head.getLong(0)
	    println(s"COUNT(*): $count")

	    // The results of SQL queries are themselves RDDs and support all normal RDD functions.  The
	    // items in the RDD are of type Row, which allows you to access each column by ordinal.
	    val rddFromSql = sqlContext.sql("SELECT key, value FROM records WHERE key < 10")

	    println("Result of RDD.map:")
	    rddFromSql.map(row => s"Key: ${row(0)}, Value: ${row(1)}").collect().foreach(println)

	    // Queries can also be written using a LINQ-like Scala DSL.
	    df.where($"key" === 1).orderBy($"value".asc).select($"key").collect().foreach(println)

	    // Write out an RDD as a parquet file.
	    df.write.parquet("pair.parquet")

	    // Read in parquet file.  Parquet files are self-describing so the schmema is preserved.
	    val parquetFile = sqlContext.read.parquet("pair.parquet")

	    // Queries can be run using the DSL on parequet files just like the original RDD.
	    parquetFile.where($"key" === 1).select($"value".as("a")).collect().foreach(println)

	    // These files can also be registered as tables.
	    parquetFile.registerTempTable("parquetFile")
	    sqlContext.sql("SELECT * FROM parquetFile").collect().foreach(println)

	    sc.stop()
	  }
	}
	// scalastyle:on println

